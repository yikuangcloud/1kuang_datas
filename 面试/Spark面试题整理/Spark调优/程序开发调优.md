## Spark面试题（七）——Spark程序开发调优
#### 1、程序开发调优 ：避免创建重复的RDD
需要对名为“hello.txt”的HDFS文件进行一次map操作，再进行一次reduce操作。也就是说，需要对一份数据执行两次算子操作。  
**错误的做法**：  
&emsp; 对于同一份数据执行多次算子操作时，创建多个RDD。//这里执行了两次textFile方法，针对同一个HDFS文件，创建了两个RDD出来，然后分别对每个RDD都执行了一个算子操作。  
这种情况下，Spark需要从HDFS上两次加载hello.txt文件的内容，并创建两个单独的RDD；//第二次加载HDFS文件以及创建RDD的性能开销，很明显是白白浪费掉的。  
```scala
    val rdd1 = sc.textFile("hdfs://master:9000/hello.txt")
    rdd1.map(...)
    val rdd2 = sc.textFile("hdfs://master:9000/hello.txt")
    rdd2.reduce(...)
```  
**正确的用法**：  
&emsp; 对于一份数据执行多次算子操作时，只使用一个RDD。  

#### 2、程序开发调优 ：尽可能复用同一个RDD
**错误的做法**：  
&emsp; 有一个<long , String>格式的RDD，即rdd1。  
&emsp; 接着由于业务需要，对rdd1执行了一个map操作，创建了一个rdd2，而rdd2中的数据仅仅是rdd1中的value值而已，也就是说，rdd2是rdd1的子集。  
```scala
    JavaPairRDD<long , String> rdd1 = ...
    JavaRDD<string> rdd2 = rdd1.map(...)
```  
&emsp; 分别对rdd1和rdd2执行了不同的算子操作。  
```scala
    rdd1.reduceByKey(...)
    rdd2.map(...)
```  
**正确的做法**：  
&emsp; rdd2的数据完全就是rdd1的子集而已，却创建了两个rdd，并对两个rdd都执行了一次算子操作。  
&emsp; 此时会因为对rdd1执行map算子来创建rdd2，而多执行一次算子操作，进而增加性能开销。  
&emsp; 其实在这种情况下完全可以复用同一个RDD。  
&emsp; 我们可以使用rdd1，既做reduceByKey操作，也做map操作。  
```scala
    JavaPairRDD<long , String> 
    rdd1 = ...rdd1.reduceByKey(...)
    rdd1.map(tuple._2...)
```  

#### 3、程序开发调优 ：对多次使用的RDD进行持久化
**正确的做法**：  
&emsp; cache()方法表示：使用非序列化的方式将RDD中的数据全部尝试持久化到内存中。  
&emsp; 此时再对rdd1执行两次算子操作时，只有在第一次执行map算子时，才会将这个rdd1从源头处计算一次。  
&emsp; 第二次执行reduce算子时，就会直接从内存中提取数据进行计算，不会重复计算一个rdd。  
```scala
    val rdd1 = sc.textFile("hdfs://192.168.0.1:9000/hello.txt").cache()
    rdd1.map(...)
    rdd1.reduce(...)
```  
&emsp; 序列化的方式可以减少持久化的数据对内存/磁盘的占用量，进而避免内存被持久化数据占用过多，从而发生频繁GC。  
```scala
    val rdd1 = sc.textFile("hdfs://192.168.0.1:9000/hello.txt")  .persist(StorageLevel.MEMORY_AND_DISK_SER)
    rdd1.map(...)
    rdd1.reduce(...)
```   
**注意**：通常不建议使用DISK_ONLY和后缀为_2的级别：因为完全基于磁盘文件进行数据的读写，会导致性能急剧降低，导致网络较大开销  

#### 4、程序开发调优 ：尽量避免使用shuffle类算子
&emsp; 如果有可能的话，要尽量避免使用shuffle类算子，最消耗性能的地方就是shuffle过程。  
&emsp; shuffle过程中，各个节点上的相同key都会先写入本地磁盘文件中，然后其他节点需要通过网络传输拉取各个节点上的磁盘文件中的相同key。而且相同key都拉取到同一个节点进行聚合操作时，还有可能会因为一个节点上处理的key过多，导致内存不够存放，进而溢写到磁盘文件中。因此在shuffle过程中，可能会发生大量的磁盘文件读写的IO操作，以及数据的网络传输操作。磁盘IO和网络数据传输也是shuffle性能较差的主要原因。  
&emsp; **尽可能避免使用reduceByKey、join、distinct、repartition等会进行shuffle的算子，尽量使用map类的非shuffle算子**。  
&emsp; 传统的join操作会导致shuffle操作。  
&emsp; 因为两个RDD中，相同的key都需要通过网络拉取到一个节点上，由一个task进行join操作。  
```scala
    val rdd3 = rdd1.join(rdd2)
```
&emsp; Broadcast+map的join操作，不会导致shuffle操作。  
&emsp; 使用Broadcast将一个数据量较小的RDD作为广播变量。  
```scala
    val rdd2Data = rdd2.collect()
    val rdd2DataBroadcast = sc.broadcast(rdd2Data)
    val rdd3 = rdd1.map(rdd2DataBroadcast...)
```  
**注意**：以上操作，建议仅仅在rdd2的数据量比较少（比如几百M，或者一两G）的情况下使用。因为每个Executor的内存中，都会驻留一份rdd2的全量数据。  

#### 5、程序开发调优 ：使用map-side预聚合的shuffle操作
&emsp; 如果因为业务需要，一定要使用shuffle操作，无法用map类的算子来替代，那么尽量使用可以map-side预聚合的算子，类似于MapReduce中的本地combiner。map-side预聚合之后，每个节点本地就只会有一条相同的key，因为多条相同的key都被聚合起来了。其他节点在拉取所有节点上的相同key时，就会大大减少需要拉取的数据数量，从而也就减少了磁盘IO以及网络传输开销。  
&emsp; 建议使用reduceByKey或者aggregateByKey算子来替代掉groupByKey算子  
<p align="center">
<img src="https://github.com/Dr11ft/BigDataGuide/blob/master/Pics/Spark%E9%9D%A2%E8%AF%95%E9%A2%98Pics/%E7%A8%8B%E5%BA%8F%E5%BC%80%E5%8F%91%E8%B0%83%E4%BC%98/5%E3%80%81%E7%A8%8B%E5%BA%8F%E5%BC%80%E5%8F%91%E8%B0%83%E4%BC%98%20%EF%BC%9A%E4%BD%BF%E7%94%A8map-side%E9%A2%84%E8%81%9A%E5%90%88%E7%9A%84shuffle%E6%93%8D%E4%BD%9C.png"/>  
<p align="center">
</p>
</p>  

#### 6、程序开发调优 ：使用高性能的算子
&emsp; 使用reduceByKey/aggregateByKey替代groupByKey              :   map-side  
&emsp; 使用mapPartitions替代普通map                                              :  函数执行频率  
&emsp; 使用foreachPartitions替代foreach                                           :  函数执行频率  
&emsp; 使用filter之后进行coalesce操作                                                :  filter后对分区进行压缩  
&emsp; 使用repartitionAndSortWithinPartitions替代repartition与sort类操作  
&emsp; repartitionAndSortWithinPartitions是Spark官网推荐的一个算子，官方建议，如果需要在repartition重分区之后，还要进行排序，建议直接使用repartitionAndSortWithinPartitions算子  

#### 7、程序开发调优 ：广播大变量
&emsp; 有时在开发过程中，会遇到需要在算子函数中使用外部变量的场景（尤其是大变量，比如100M以上的大集合），那么此时就应该使用Spark的广播（Broadcast）功能来提升性能。  
&emsp; 默认情况下，Spark会将该变量复制多个副本，通过网络传输到task中，此时每个task都有一个变量副本。如果变量本身比较大的话（比如100M，甚至1G），那么大量的变量副本在网络中传输的性能开销，以及在各个节点的Executor中占用过多内存导致的频繁GC，都会极大地影响性能。  
&emsp; 广播后的变量，会保证每个Executor的内存中，只驻留一份变量副本，而Executor中的task执行时共享该Executor中的那份变量副本。  

#### 8、程序开发调优 ：使用Kryo优化序列化性能
&emsp; 1）在算子函数中使用到外部变量时，该变量会被序列化后进行网络传输。  
&emsp; 2）将自定义的类型作为RDD的泛型类型时（比如JavaRDD，Student是自定义类型），所有自定义类型对象，都会进行序列化。因此这种情况下，也要求自定义的类必须实现Serializable接口。   
&emsp; 3）使用可序列化的持久化策略时（比如MEMORY_ONLY_SER），Spark会将RDD中的每个partition都序列化成一个大的字节数组。  
Spark默认使用的是Java的序列化机制，你可以使用Kryo作为序列化类库，效率要比Java的序列化机制要高  
```scala
// 创建SparkConf对象。
val conf = new SparkConf().setMaster(...).setAppName(...)
// 设置序列化器为KryoSerializer。
conf.set("spark.serializer", "org.apache.spark.serializer.KryoSerializer")
// 注册要序列化的自定义类型。
conf.registerKryoClasses(Array(classOf[MyClass1], classOf[MyClass2]))
```  

#### 9、程序开发调优 ：分区Shuffle优化
&emsp; 当遇到userData和events进行join时，userData比较大，而且join操作比较频繁，这个时候，可以先将userData调用了 partitionBy()分区，可以极大提高效率。  
&emsp; cogroup()、 groupWith()、join()、leftOuterJoin()、rightOuterJoin()、groupByKey()、reduceByKey()、 combineByKey() 以及 lookup()等都能够受益  
<p align="center">
<img src="https://github.com/Dr11ft/BigDataGuide/blob/master/Pics/Spark%E9%9D%A2%E8%AF%95%E9%A2%98Pics/%E7%A8%8B%E5%BA%8F%E5%BC%80%E5%8F%91%E8%B0%83%E4%BC%98/9%E3%80%81%E7%A8%8B%E5%BA%8F%E5%BC%80%E5%8F%91%E8%B0%83%E4%BC%98%20%EF%BC%9A%E5%88%86%E5%8C%BAShuffle%E4%BC%98%E5%8C%96.png"/>  
<p align="center">
</p>
</p>  

**总结**：如果遇到一个RDD频繁和其他RDD进行Shuffle类操作，比如 cogroup()、 groupWith()、join()、leftOuterJoin()、rightOuterJoin()、groupByKey()、reduceByKey()、 combineByKey() 以及 lookup()等，那么最好将该RDD通过partitionBy()操作进行预分区，这些操作在Shuffle过程中会减少Shuffle的数据量  

#### 10、程序开发调优 ：优化数据结构
Java中，有三种类型比较耗费内存：  
&emsp; 1）对象，每个Java对象都有对象头、引用等额外的信息，因此比较占用内存空间。   
&emsp; 2）字符串，每个字符串内部都有一个字符数组以及长度等额外信息。         
&emsp; 3）集合类型，比如HashMap、LinkedList等，因为集合类型内部通常会使用一些内部类来封装集合元素，比如Map.Entry  
&emsp; Spark官方建议，在Spark编码实现中，特别是对于算子函数中的代码，尽量不要使用上述三种数据结构，尽量使用字符串替代对象，使用原始类型（比如Int、Long）替代字符串，使用数组替代集合类型，这样尽可能地减少内存占用，从而降低GC频率，提升性能。  
















